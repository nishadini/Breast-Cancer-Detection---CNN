# -*- coding: utf-8 -*-
"""Xception.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1j90vySlauD7uIfo4hf27PXmjASxWb_Re
"""

from google.colab import files
import zipfile
import io

uploaded = files.upload()
tnew = zipfile.ZipFile(io.BytesIO(uploaded['tnew.zip']), 'r')
tnew.extractall()


uploaded = files.upload()
vnew = zipfile.ZipFile(io.BytesIO(uploaded['vnew.zip']), 'r')
vnew.extractall()

uploaded = files.upload()
testnew = zipfile.ZipFile(io.BytesIO(uploaded['testnew.zip']), 'r')
testnew.extractall()

# Commented out IPython magic to ensure Python compatibility.
import os
import PIL
import tensorflow as tf
import numpy as np
import os
import keras
from IPython.display import Image
import glob  #Return a possibly-empty list of path names that match pathname, which must be a string containing a path specification
import matplotlib.pyplot as plt   # Python 2D plotting library which produces publication quality figures. Combine numpy and matplotlib
from keras.applications.xception import Xception,preprocess_input
from keras.models import Model                #groups layers into an object with training and inference features.
from keras.layers import Dense, GlobalAveragePooling2D
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import SGD, Adam
from tensorflow.keras.layers import Dropout
from keras.layers import Input

from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
import itertools
import shutil
import matplotlib.pyplot as plt
# %matplotlib inline
import pandas as pd

IM_WIDTH, IM_HEIGHT = 299,299

def get_nb_files(directory):
  """Get number of files by searching directory recursively"""
  if not os.path.exists(directory):
    return 0
  cnt = 0
  for r, dirs, files in os.walk(directory):
    for dr in dirs:
      cnt += len(glob.glob(os.path.join(r, dr + "/*")))
  return cnt

#use to get the no.of classes or folders as abnormal normal

def setup_to_transfer_learn(model, base_model):
  """Freeze all layers and compile the model - Transfer Learning"""
  for layer in base_model.layers: 
    layer.trainable = False
  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

 #transfer learning method - all layers freeze in base model 


def add_new_last_layer(base_model, nb_classes):
 
  x = base_model.output
  x = GlobalAveragePooling2D()(x)
  x = Dense(512, activation='relu')(x)
  x = Dropout(0.5)(x)
  x = Dense(60, activation='relu')(x)
  x = Dropout(0.3)(x)
  predictions = Dense(nb_classes, activation='softmax')(x) #new softmax layer give the no.of classes at the end 
  model = tf.keras.Model(inputs=base_model.input, outputs=predictions)
  return model
  
#create new model adding new layers

def setup_to_finetune(model):
 
  for layer in model.layers[:-8]:
     layer.trainable = False
  """for layer in model.layers[25:]:
     layer.trainable = True"""

  #model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])
  model.compile(Adam(lr=0.0000001),loss='binary_crossentropy', metrics=['accuracy'])

 #fine tuning - new model training 

def train():
  """Use transfer learning and fine-tuning to train a network on a new dataset"""

  tnew = 'tnew'
  vnew = 'vnew'
  testnew= 'testnew'

  nb_epoch = 150
  batch_size= 1

  nb_train_samples = get_nb_files(tnew)
  nb_val_samples = get_nb_files(vnew)
  nb_test_samples = get_nb_files(testnew)
  nb_classes = len(glob.glob(tnew + "/*"))

  steps_per_epoch= np.ceil (nb_train_samples/batch_size)
  steps_per_epoch_val = np.ceil (nb_val_samples/batch_size)
  steps_per_epoch_test = np.ceil (nb_test_samples/batch_size)

  # data prep
  train_datagen =  ImageDataGenerator(preprocessing_function=preprocess_input, rotation_range=5,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.5,horizontal_flip=False )
  val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,rotation_range=5,width_shift_range=0.2,height_shift_range=0.2,shear_range=0.2,zoom_range=0.5, horizontal_flip=False)
  test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

  train_generator = train_datagen.flow_from_directory(tnew,target_size=(IM_WIDTH, IM_HEIGHT),batch_size=batch_size)
  validation_generator = val_datagen.flow_from_directory(vnew,target_size=(IM_WIDTH, IM_HEIGHT),batch_size=batch_size)
  test_generator = test_datagen.flow_from_directory( testnew,target_size=(IM_WIDTH, IM_HEIGHT),batch_size=batch_size)
 
  # setup model
  base_model = keras.applications. xception. Xception(weights="imagenet", include_top=False)

  print('Base Model')
  base_layers = len(base_model.layers)
  print (base_layers)
  print(base_model.summary())
  model = add_new_last_layer(base_model, nb_classes)
  print('New Model')
  new_layers = len(model.layers)
  print (new_layers)
  print(model.summary())

  # transfer learning
  setup_to_transfer_learn(model, base_model)

  history_tl = model.fit_generator(generator=train_generator,
                    steps_per_epoch=steps_per_epoch,
                    epochs=nb_epoch,
                    validation_data=validation_generator,
                    validation_steps=steps_per_epoch_val,
                    shuffle = True,
                    verbose=1
                    )
  
  # fine-tuning
  setup_to_finetune(model)

  history_ft = model.fit_generator(generator=train_generator,
                        steps_per_epoch=steps_per_epoch,
                        epochs=nb_epoch,
                        shuffle = True,
                        validation_data=validation_generator,
                        validation_steps=steps_per_epoch_val,
                        verbose=1
                        )
  
  train_accuracy = model.evaluate_generator(train_generator, steps=steps_per_epoch)
  validation_accuracy = model.evaluate_generator(validation_generator, steps=steps_per_epoch_val)
  test_accuracy = model.evaluate_generator(test_generator, steps=steps_per_epoch_test)
  #test_loss, test_acc = model.evaluate(test)
  
  model.save('interim.h5')
  print('Train Accuracy', train_accuracy[1])
  print('Validation Accuracy', validation_accuracy[1])
  print('Test Accuracy', test_accuracy[1])

  print(validation_generator.class_indices)

  #Matrix inputs

  test_labels = validation_generator.classes
  predictions = model.predict_generator(validation_generator, steps=steps_per_epoch_test, verbose=1)

  test1_labels = test_generator.classes
  predictions1 = model.predict_generator(test_generator, steps=steps_per_epoch_test, verbose=1)
  
  return history_tl, history_ft,test_labels,predictions,test1_labels,predictions1
  
if __name__=="__main__":  
  history_tl, history_ft,test_labels,predictions,test1_labels,predictions1 = train()
  
  acc = history_tl.history['accuracy']
  val_acc = history_tl.history['val_accuracy']
  loss = history_tl.history['loss']
  val_loss =history_tl.history['val_loss']
  epochs = range(len(acc))
  plt.plot(epochs,acc,'g-', label='Training acc')
  plt.plot(epochs,val_acc, 'b-',label='validation acc')
  plt.title('Training and validation accuracy - TL')

  plt.figure()
  plt.plot(epochs, loss,'g-', label='Training loss')
  plt.plot(epochs, val_loss,'b-', label='Validaion loss')
  plt.title('Training and validation loss - TL')
  plt.show()
  plt.savefig('plot')

  print(history_ft.history.keys())
  acc1 = history_ft.history['accuracy']
  val_acc1 = history_ft.history['val_accuracy']
  loss1 = history_ft.history['loss']
  val_loss1 = history_ft.history['val_loss']
  epochs1 = range(len(acc))

  plt.plot(epochs1, acc1,'g-', label='Training acc')
  plt.plot(epochs1,val_acc1, 'b-', label='Validation acc')
  plt.title('Training and validation accuracy - FT')

  plt.figure()
  plt.plot( epochs1,loss1, 'g-', label='Training loss')
  plt.plot( epochs1,val_loss1,'b-', label='Validaion loss')
  plt.title('Training and validation loss - FT')
  plt.show()
  plt.savefig('plot2')

  #confusion matrix

  def plot_confusion_matrix(cm, nb_classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

    print(cm)

    plt.figure(figsize=(5,5))
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(nb_classes))
    plt.xticks(tick_marks, nb_classes, rotation=45)
    plt.yticks(tick_marks, nb_classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.tight_layout()

  # argmax returns the index of the max value in a row
  cm = confusion_matrix(test_labels, predictions.argmax(axis=1))

  # Define the labels of the class indices. These need to match the 
  # order shown above.
  cm_plot_labels = ['Abnormal','Normal']

  plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')

  # Get the index of the class with the highest probability score
  y_pred = np.argmax(predictions, axis=1)

  # Get the labels of the test images.
  y_true = test_labels

  from sklearn.metrics import classification_report

  # Generate a classification report
  report = classification_report(y_true, y_pred, target_names=cm_plot_labels)

  print(report)

  # argmax returns the index of the max value in a row
  cm1 = confusion_matrix(test1_labels, predictions1.argmax(axis=1))

  # Define the labels of the class indices. These need to match the 
  # order shown above.
  cm1_plot_labels = ['Abnormal','Normal']

  plot_confusion_matrix(cm1, cm1_plot_labels, title='Confusion Matrix')

  # Get the index of the class with the highest probability score
  y_pred1 = np.argmax(predictions1, axis=1)

  # Get the labels of the test images.
  y_true1 = test1_labels

  # Generate a classification report
  report1 = classification_report(y_true1, y_pred1, target_names=cm1_plot_labels)

  print(report1)